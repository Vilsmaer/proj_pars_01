# parse_all_products.py ‚Äî –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å–ø–∏—Å–∫–∞ URL

import requests
from bs4 import BeautifulSoup
import csv
import re
import time

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
}
# –£–ö–ê–ó–´–í–ê–ï–ú –ü–†–Ø–ú–û–ô –ü–£–¢–¨ –ö –§–ê–ô–õ–£
INPUT_FILE = r"D:\ANKO\proj_pars_01\filtered_product_urls.txt"
#OUTPUT_FILE = "all_products.csv"  # –º–æ–∂–Ω–æ —Ç–æ–∂–µ —Å–¥–µ–ª–∞—Ç—å –ø–æ–ª–Ω—ã–º, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
OUTPUT_FILE = r"D:\ANKO\proj_pars_01\all_products.csv"


# === –§–£–ù–ö–¶–ò–Ø: –ò–∑–≤–ª–µ—á—å —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ ===
def extract_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    clean = re.sub(r'[^\d]', '', price_text)  # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
    try:
        return float(clean) if clean else 0.0
    except ValueError:
        return 0.0


# === –§–£–ù–ö–¶–ò–Ø: –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞ ===
def parse_product_page(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    data = {"url": url}

    # üîπ –ù–ê–ó–í–ê–ù–ò–ï ‚Äî –∏–∑ <h1>
    title_tag = soup.select_one('h1')
    data["name"] = title_tag.get_text(strip=True) if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

    # üîπ –¶–ï–ù–ê ‚Äî –∏—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É .price_value
    price_tag = soup.select_one('.price_value')
    price_text = price_tag.get_text(strip=True) if price_tag else ""
    data["price"] = extract_price(price_text)
    data["price_raw"] = price_text

    # üîπ –ù–ê–õ–ò–ß–ò–ï ‚Äî –∏—â–µ–º —Ç–µ–∫—Å—Ç "–í –Ω–∞–ª–∏—á–∏–∏: X"
    availability_tag = soup.find(string=re.compile(r'–í –Ω–∞–ª–∏—á–∏–∏'))
    if availability_tag:
        match = re.search(r'–í –Ω–∞–ª–∏—á–∏–∏:\s*(\d+)', availability_tag)
        data["in_stock"] = int(match.group(1)) if match else 0
    else:
        data["in_stock"] = 0

    # üîπ –¢–ê–ë–õ–ò–¶–ê –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö
    rows = soup.select('table tr')
    for row in rows:
        cols = row.find_all('td')
        if len(cols) >= 2:
            key = cols[0].get_text(strip=True).rstrip(':')
            value = cols[1].get_text(strip=True)
            # –û—á–∏—â–∞–µ–º –∫–ª—é—á –¥–ª—è CSV (—É–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)
            key_clean = re.sub(r'[^\w\s]', '', key).strip().replace(' ', '_').lower()
            data[key_clean] = value

    return data


# === –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===
def main():
    # –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f" –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {INPUT_FILE}")
        print(" –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø—É—Ç—å —É–∫–∞–∑–∞–Ω –≤–µ—Ä–Ω–æ.")
        return

    print(f" –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
    all_products = []

    for i, url in enumerate(urls, 1):
        print(f" [{i}/{len(urls)}] –ü–∞—Ä—Å–∏–º: {url}")

        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            if response.status_code == 200:
                product = parse_product_page(response.text, url)
                all_products.append(product)
                print(f" –£—Å–ø–µ—à–Ω–æ: {product['name']}")
            else:
                print(f" –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (HTTP {response.status_code})")
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")

        # –í–µ–∂–ª–∏–≤–∞—è –ø–∞—É–∑–∞ ‚Äî –Ω–µ –Ω–∞–≥—Ä—É–∂–∞–µ–º —Å–µ—Ä–≤–µ—Ä
        time.sleep(1)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –≤ CSV
    if all_products:
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏ (–Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤)
        fieldnames = set()
        for p in all_products:
            fieldnames.update(p.keys())
        fieldnames = sorted(fieldnames)

        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_products)

        print(f"\ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_FILE}")
        print(f" –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
    else:
        print(" –ù–∏ –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –Ω–µ –±—ã–ª —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω")

    print(" –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()